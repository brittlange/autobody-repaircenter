---
output:
  html_document:
    toc: yes
    toc_depth: 3
    df_print: paged
    toc_float: true
title: Team 45 - Auto Repair Shop Demand Planning
toc-title: Auto Repair Shop Demand Planning
bibliography: "`r here::here('Other Resources', 'Final report references.bib')`"
nocite: '@*'
link-citations: yes
csl: https://raw.githubusercontent.com/philipbelesky/Markdown-Citation-Style-Languages/master/mla-url.csl
urlcolor: blue
---
\newpage
MGT 6203: Data Analytics in Business\
`r format(Sys.Date(), '%B %d, %Y')`\

```{r include=FALSE}
library(scales)
body_shop_count <- 710
```

# Introduction
## Team
### Number
45

### Members
| Name                | Email                   |
|:-------------------:|:-----------------------:|
| Lisa Chille         | lchille@gatech.edu      |
| Nicholas Cunningham | ncunningham8@gatech.edu |
| Jeffrey Hedberg     | jhedberg3@gatech.edu    |
| Brittany Lange      | blange9@gatech.edu      |
| Delband Taha        | dtaha@gatech.edu        |

## Background information
A hypothetical auto repair shop may struggle with demand forecasting since no one plans on getting into an accident. Therefore, none of their customers have any prior intent of availing their services before finding themselves with the challenge of getting their car repaired^[We assume, of course, that all of their customers have normal psychological patterns (@NormalPsychology)]. By statistically analyzing and making inferences from collision data, the shop stands a chance at getting an understanding of their customers' patterns and can therefore ready themselves by making the appropriate hiring/firing and physical expansion/subletting decisions as necessary. With a little more effort, they can even learn which demographic(s) to raise awareness with such that in the event of a crisis, this repair shop is top of mind for the affected parties.

This project aims at supporting the annual planning process of a hypothetical repair shop in New York City through careful data analysis and inference that leads to recommendations for the auto repair shop owner(s).

## Motivation
Our objective of this project is to use data compiled by New York City police officers (NYPD) detailing New York motor vehicle collision reports to help a fictitious auto repair center in New York City estimate the volume of incoming vehicles they can expect to repair in the coming year, assuming their market share is known. Our analysis will help this repair center predict staffing levels that they will need to maintain and identify potential opportunities for expansion.

Additionally, we analyze the demographics of those involved in collisions and identify the groups that make a significant contribution to car collisions. We propose and measure the impact of a marketing campaign for this repair center. We also analyze the impact of COVID-19 on this industry sector.

## Assumptions
We assume equal market share for the `r body_shop_count` (@DMVBodyShopCensus) body shops found in New York City. This gives us an average market share of `r percent(1/body_shop_count, .0001)` per shop, which we will use to estimate the volume of incoming vehicles they can expect.

## Hypotheses
After our initial data exploration, we developed a few hypotheses:

### Primary hypotheses
1. Be able to estimate the monthly demand for a fictitious auto shop in New York state by extrapolating the monthly car crash volume with reasonable accuracy.
2. Identify a demographic that comprises the largest market segment for motor vehicle repairs after an accident; from our preliminary investigation, we suspect that young men comprise this demographic.

### Secondary hypotheses
1. Be able to assert the impact of advertising expenditure on the auto shop's bottom line.
2. Be able to quantify the impact of COVID-19 on demand in this sector.

## Literature review
### Demographics
In their research, Regev et. al. (@REGEV2018131) evaluated the crash risk of drivers of different ages and genders _adjusting for travel exposure_ and found that drivers between the ages of 21 and 29 carry the highest level of risk for being in a car crash. This contrasts prior research that suggested that teenagers and elderly drivers carry the highest risk for car collisions since that research did not control for the effect of travel exposure (@REGEV2018131, 131). In the team's initial discussions, we hypothesised that the data would show that teenagers and elderly folks have the highest rate of car collisions. This article broadened our perspective and set us up to let the data guide us.

### Location
He et. al. (@MITResearch) developed a technique to predict where accidents happen. This work would allow policymakers to install traffic calming devices in areas that are predetermined to be risky. While we did not build directly on their work, we were inspired by the advanced models that they built.

### COVID-19
Li and Zhao (@COVID19Effect) determined that while the overall volume of car collisions has plummeted since the start of the pandemic, cyclists' fatalities have tripled when the quantity of traffic is controlled for. This presents an area of further research for us. To dig deeper into the true impact of the pandemic, we would have to collect or find data that we could then merge with our existing data sets to glean further insights. Due to the fact that our time in this course is limited, we considered this to be out of scope for this project.

## Approach
To perform this analysis, we take several steps:

### Data wrangling
In this step, we will read into memory, and clean the data at hand. We use 3 data sets:

1. Data detailing the collisions, `Crashes`,
2. Data detailing the vehicles involved in collisions, `Vehicles`, and
3. Data detailing the people involved in the collisions, `Person`.

After the data is read, we display a few summaries to give the reader a sense of the scope in which we are dealing. We then merge the 3 data sets so that we can perform analysis smoothly.

### Data exploration
After the initial wrangling is done, we then explore the data by making a few plots. One effect we reflect on is the effect of COVID-19 on collisions in New York City.

### Feature selection
We use several strategies to perform analysis with the features that have the most predictive power:

1. Manual predictor selection
2. LASSO
3. Elastic net

### Modeling
We split the data into 3 sets: training, validation and test data sets. We then take advantage of 5 different modeling techniques:

1. Linear regression,
2. Random forest, and
3. Classification and regression tree, and
4. Gradient Boosted Trees, and
5. Ensemble of all 4 models above.

For each, we visualize the output and then analyze its performance on the validation data set. We use $R^2$ to analyze model performance.

### Model comparison
The objective here is to summarise the models' performance metrics to get an instant idea of which model is the best. Note that we compare the models' performances on the validation data set.

### Prediction
Finally, we predict the collision volume on the test data set to estimate the chosen model's performance. This prediction also serves as the basis for our recommendations for the auto shop.

### Effect of a marketing campaign
We then perform an analysis to understand the impact of a marketing campaign on the auto repair shop's business.

### Effect of COVID-19
We also evaluate the effect of COVID-19 on the amount of business that the shop can get.

### Conclusion
We then draw conclusions based on the results obtained above.

## Useful libraries
We import necessary libraries. These libraries will be used as follows:  
```{r table2, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
package_table <- "
|     Library    |               Use              |
|:--------------:|:------------------------------:|
|     `dplyr`    |        Data manipulation       |
|     `knitr`    |       RMarkdown knitting       |
|  `kableExtra`  |          Kable tables          |
|    `plotly`    |            Plotting            |
|   `lubridate`  |        Date manipulation       |
| `randomForest` |  Building Random Forest models |
|     `rpart`    |      Building CART models      |
|  `rpart.plot`  | Plotting regression trees built|
|    `xgboost`   |  Building Boosted Tree models  |
"
cat(package_table)
```

```{r, message = FALSE, warning = FALSE}
library(dplyr)
library(knitr)
library(kableExtra)
library(plotly)
library(lubridate)
library(randomForest)
library(rpart)
library(rpart.plot)
library(xgboost)
```

# Data
## Summaries
Below, we import data from 3 sources and view the raw summaries. The data are related to each other as follows:
[!Data relationships](../Data/Relationships.png)

These functions will be reused in our endeavour to read and summarise the data at hand.
```{r}
read_data <- function(path) {
  data <- read.csv(path, stringsAsFactors = FALSE)
  colnames(data)[colnames(data) == 'CRASH.DATE'] <- 'CRASH_DATE'
  data$CRASH_DATE <- as.Date(data$CRASH_DATE, "%m/%d/%Y")
  data
}
tabulate_collisions <- function(data) {
  kable(t(summary(data))) %>% kable_classic(full_width = TRUE, html_font = "Cambria",
                                               font_size = 14)
}
```
### `Crashes`
```{r, fig.width = 10, cache = FALSE}
crashes_df <- read_data('../Data/Crashes.csv') #1,896,229 x 29
tabulate_collisions(crashes_df)
```

### `Person`
```{r, fig.width = 10, cache = FALSE}
person_df <- read_data('../Data/Person.csv') #4,692,054 x 21
tabulate_collisions(person_df)
```

### `Vehicles`
```{r, fig.width = 10, cache = FALSE}
vehicles_df <- read_data('../Data/Vehicles.csv') #3,704,406 x 25
tabulate_collisions(vehicles_df)
```

Note the large sizes of the data sets we are dealing with.

## Wrangling
As a result of a traffic safety initiative to eliminate traffic fatalities, the NYPD replaced its record management system with an electronic one, (FORMS), in 2016 (@DataBackground). We see this change reflected in the chart below.
```{r, fig.width = 10, cache = FALSE}
monthly_agg1 <- (person_df %>% 
                   mutate(yr_mo = paste0(substr(CRASH_DATE,1,4),'-',substr(CRASH_DATE,6,7))) %>% 
                   group_by(yr_mo) %>% 
                   summarise(n = n()) %>% 
                   arrange(yr_mo)
                 )
plot_ly(x=monthly_agg1$yr_mo, y=monthly_agg1$n, type='bar') %>%
  layout(title = "Crash Data by Month from 2012-2022", xaxis = list(title = 'Month'), yaxis = list(title = 'Number of Crashes'))
```
Note that the amount of data collected from March 2016 on greatly surpasses the amount previously collected. To control for the change in data recording systems, we will use data collected after January 1st, 2017 for full year modeling.

We are now ready to filter the data to select columns of interest. We will also combine data from our three sources into one place, using their common factors. A summary can be found below.
```{r, fig.width = 10, cache = FALSE}
combined_df <- (crashes_df %>% 
                  select(-c(CRASH_DATE, CRASH.TIME, CONTRIBUTING.FACTOR.VEHICLE.1, CONTRIBUTING.FACTOR.VEHICLE.2,
                            CONTRIBUTING.FACTOR.VEHICLE.3, CONTRIBUTING.FACTOR.VEHICLE.4, CONTRIBUTING.FACTOR.VEHICLE.5,
                            VEHICLE.TYPE.CODE.4, VEHICLE.TYPE.CODE.5,
                            NUMBER.OF.PEDESTRIANS.INJURED, NUMBER.OF.PEDESTRIANS.KILLED,
                            NUMBER.OF.CYCLIST.INJURED, NUMBER.OF.CYCLIST.KILLED,
                            ON.STREET.NAME, CROSS.STREET.NAME, OFF.STREET.NAME)) %>% 
                  inner_join(vehicles_df %>% 
                               filter(!is.na(VEHICLE_ID)) %>% 
                               select(-c(CRASH_DATE, CRASH_TIME, VEHICLE_ID))
                             , by = 'COLLISION_ID') %>% 
                  inner_join(person_df %>% 
                               select(-c(UNIQUE_ID, CONTRIBUTING_FACTOR_1, CONTRIBUTING_FACTOR_2))
                             , by= c('COLLISION_ID'='COLLISION_ID', 'UNIQUE_ID'='VEHICLE_ID')) %>% 
                  filter((PED_ROLE %in% c('Driver'))) %>%  #drivers only
                  filter((CRASH_DATE >= '2017-01-01') & (CRASH_DATE < '2021-12-01')) %>% #only from 2017-01-01 to 2021-11-30
                  filter(PERSON_AGE > 14 & PERSON_AGE< 101) %>% 
                  filter(PERSON_SEX == 'F' | PERSON_SEX=='M') %>% 
                  mutate(MONTH = substr(CRASH_DATE,6,7)) %>% 
                  mutate(TIMESTEP = as.period(interval(as.Date('2017-01-01'), CRASH_DATE)) %/% months(1)) %>% 
                  mutate(yr_mo = paste0(substr(CRASH_DATE,1,4),'-',substr(CRASH_DATE,6,7))) %>%
                  select(COLLISION_ID, CRASH_DATE, PERSON_AGE, PERSON_SEX, MONTH, TIMESTEP, yr_mo) %>% 
                  arrange(CRASH_DATE)
                )
kable(t(summary(combined_df))) %>% kable_classic(full_width = TRUE, html_font = "Cambria", font_size = 14)
```

Now, our data set is ready for exploration.

# Data Exploration
We create our aggregate of volume data available for modeling and visualize it with a plot. It is interesting to note the impact of COVID-19 on crash volume beginning in March 2020.
```{r, fig.width = 10, cache = FALSE}
monthly_agg2 <- (combined_df %>% 
                   group_by(TIMESTEP, yr_mo, MONTH) %>% 
                   summarise(n = n(), .groups = 'drop') %>% 
                   arrange(TIMESTEP, yr_mo, MONTH)
                 )
plot_ly(x=monthly_agg2$yr_mo, y=monthly_agg2$n, type='bar') %>%
  layout(title = "Crash Data by Month from 2017-2021", xaxis = list(title = 'Month'), yaxis = list(title = 'Number of Crashes'))
```

# Modeling
## Splitting into training, validation, and testing data sets
We also created a coarse aggregate for modeling with feature groups and separate our data into training, validation, and test data sets. Since we are dealing with a temporal model, we will select 2017-01 to 2019-12 for training, 2020-01 to 2020-12 for validation, and for 2021-01 to 2021-11 testing. A random split would be nonsensical for our purposes.  After hyper parameters for each model have been tuned using training and validation data sets, the models will be re-trained using the training + validation data sets.  This ensures that we maximize the data for learning, but also that validation steps are completed properly.
```{r, fig.width = 10, cache = FALSE}
monthly_agg3 <- (combined_df %>% 
                   group_by(TIMESTEP, yr_mo, MONTH, PERSON_SEX, PERSON_AGE) %>% 
                   summarise(n = n(), .groups = 'drop') %>% 
                   arrange(TIMESTEP, yr_mo, MONTH, PERSON_SEX, PERSON_AGE)
)

#Create Training and Test data sets
train_df <- monthly_agg3 %>% filter(TIMESTEP <= 35) # 2017-01 to 2019-12
val_df <- monthly_agg3 %>% filter((TIMESTEP > 35) & (TIMESTEP <= 47)) # 2020-01 to 2020-12
test_df <- monthly_agg3 %>% filter(TIMESTEP > 47) # 2021-01 to 2021-11

#Create Train_Val data sets (final model after Train/Val hyper parameter tuning)
train_val_df <- monthly_agg3 %>% filter(TIMESTEP <= 47) # 2017-01 to 2020-12
```

```{r, fig.width = 10, cache = FALSE, echo = FALSE}
monthly_agg_data_splits_df <- (combined_df %>% 
                              group_by(TIMESTEP, yr_mo, MONTH) %>% 
                              summarise(n = n(), .groups = 'drop') %>% 
                              arrange(TIMESTEP, yr_mo, MONTH) %>% 
                              mutate(Partition = ifelse(TIMESTEP <= 35,'Train',ifelse(TIMESTEP <= 47,'Val', 'Test')))
                            )

plot_ly(x=monthly_agg_data_splits_df$yr_mo, y=monthly_agg_data_splits_df$n, type='bar', color=monthly_agg_data_splits_df$Partition) %>%
  layout(title = "Crash Data by Month from 2017-2021", xaxis = list(title = 'Month'), yaxis = list(title = 'Number of Crashes'))
```
## Linear Regression
### Training
```{r, fig.width = 10, cache = FALSE}
#Train Linear Model
lm_model <- lm(data = train_df, formula = n ~ TIMESTEP + MONTH + PERSON_AGE + PERSON_SEX )
summary(lm_model)
SST_lm_train <- sum((train_df$n - mean(train_df$n))^2)
SSE_lm_train <- sum((train_df$n - predict(lm_model, newdata = train_df))^2)
R_sq_lm_train <- 1-(SSE_lm_train/SST_lm_train)
R_sq_lm_train #0.5799

#Linear Model Validation Metrics
SST_lm_val <- sum((val_df$n - mean(val_df$n))^2)
SSE_lm_val <- sum((val_df$n - predict(lm_model, newdata = val_df))^2)
R_sq_lm_val <- 1-(SSE_lm_val/SST_lm_val)
R_sq_lm_val #-1.154703

#Train_Val Linear Model
lm_model_2 <- lm(data = train_val_df, formula = n ~ TIMESTEP + MONTH + PERSON_AGE + PERSON_SEX )
summary(lm_model_2)
SST_lm_train_val <- sum((train_val_df$n - mean(train_val_df$n))^2)
SSE_lm_train_val <- sum((train_val_df$n - predict(lm_model_2, newdata = train_val_df))^2)
R_sq_lm_train_val <- 1-(SSE_lm_train_val/SST_lm_train_val)
R_sq_lm_train_val #0.545718
```

### Visualisation
```{r, fig.width = 10, cache = FALSE}
plot_ly(x=train_val_df$yr_mo, y=(predict(lm_model_2, newdata = train_val_df)-train_val_df$n), type='scatter', mode='markers') %>% 
  layout(title = 'Plot of Linear Model Residuals vs yr_mo for Train_Val Data',
         xaxis = list(title = 'yr_mo'),
         yaxis = list(title = 'LM Residuals', rangemode = "tozero"))
```
### Test Metrics
```{r, fig.width = 10, cache = FALSE}
# Verify performance on Test dataset
SST_lm_test <- sum((test_df$n - mean(test_df$n))^2)
SSE_lm_test <- sum((test_df$n - predict(lm_model_2, newdata = test_df))^2)
R_sq_lm_test <- 1-(SSE_lm_test/SST_lm_test)
R_sq_lm_test #0.08348344
```
Our linear model with training + validation data had an R-squared value of 0.55, while it's performance on test data dropped the R-squared value to 0.08.

## Random forest
### Training
```{r, fig.width = 10, cache = FALSE}
# #Train & Val Random Forest Model - Loop - Run only once to get optimal settings
# rf_results <- data.frame()
# for(i in seq.int(1, 200)){
#   set.seed(12345)
#   rf_model <- randomForest(n ~ TIMESTEP + MONTH + PERSON_AGE + PERSON_SEX, data=train_df, importance=TRUE, ntree=i)  #
#   summary(rf_model)
#   var_importance_df <- data.frame(importance(rf_model)) %>% rename('PCT_IncMSE'='X.IncMSE') %>% arrange(desc(PCT_IncMSE))
#   var_importance_df
#   SST_rf_train <- sum((train_df$n - mean(train_df$n))^2)
#   SSE_rf_train <- sum((train_df$n - predict(rf_model, newdata = train_df))^2)
#   R_sq_rf_train <- 1-(SSE_rf_train/SST_rf_train)
#   R_sq_rf_train #0.8670912
# 
#   #Random Forest Model Validation Metrics
#   SST_rf_val <- sum((val_df$n - mean(val_df$n))^2)
#   SSE_rf_val <- sum((val_df$n - predict(rf_model, newdata = val_df))^2)
#   R_sq_rf_val <- 1-(SSE_rf_val/SST_rf_val)
#   R_sq_rf_val #-0.5731295
# 
#   rf_results <- rf_results %>% bind_rows(data.frame(i=i, R_sq_rf_train=R_sq_rf_train, R_sq_rf_val=R_sq_rf_val))
# }
# 
# rf_results %>% filter(rf_results$R_sq_rf_val == max(rf_results$R_sq_rf_val))
# # i R_sq_rf_train R_sq_rf_val
# # 1 41     0.8668467    -0.49019

# Best Model from Train & Val
set.seed(12345)
rf_model <- randomForest(n ~ TIMESTEP + MONTH + PERSON_AGE + PERSON_SEX, data=train_df, importance=TRUE, ntree=41)  #41
SST_rf_train <- sum((train_df$n - mean(train_df$n))^2)
SSE_rf_train <- sum((train_df$n - predict(rf_model, newdata = train_df))^2)
R_sq_rf_train <- 1-(SSE_rf_train/SST_rf_train)
R_sq_rf_train #0.8668467

#Random Forest Model Validation Metrics
SST_rf_val <- sum((val_df$n - mean(val_df$n))^2)
SSE_rf_val <- sum((val_df$n - predict(rf_model, newdata = val_df))^2)
R_sq_rf_val <- 1-(SSE_rf_val/SST_rf_val)
R_sq_rf_val #-0.49019

# Train_Val RF model
set.seed(12345)
rf_model_2 <- randomForest(n ~ TIMESTEP + MONTH + PERSON_AGE + PERSON_SEX, data=train_val_df, importance=TRUE, ntree=41)  #41
rf_var_importance_df <- data.frame(importance(rf_model_2)) %>% rename('PCT_IncMSE'='X.IncMSE') %>% arrange(desc(PCT_IncMSE))
SST_rf_train_val <- sum((train_val_df$n - mean(train_val_df$n))^2)
SSE_rf_train_val <- sum((train_val_df$n - predict(rf_model_2, newdata = train_val_df))^2)
R_sq_rf_train_val <- 1-(SSE_rf_train_val/SST_rf_train_val)
R_sq_rf_train_val #0.8480507
```

### Visualisation
```{r, fig.width = 10, cache = FALSE}
# Make plot for Random Forest Feature Importance
plot_ly(x=rf_var_importance_df$PCT_IncMSE , y=reorder(rf_var_importance_df %>% row.names(), rf_var_importance_df$PCT_IncMSE ), type='bar', orientation = 'h') %>%
  layout(title = "Random Forest Feature Importance", xaxis = list(title = '% Increase MSE'), yaxis = list(title = 'Feature'))

# Make plot for Random Forest Residuals
plot_ly(x=train_val_df$yr_mo, y=(predict(rf_model_2, newdata = train_val_df)-train_val_df$n), type='scatter', mode='markers') %>%
  layout(title = 'Plot of Random Forest Model Residuals vs yr_mo for Train_Val Data',
         xaxis = list(title = 'yr_mo'),
         yaxis = list(title = 'RF Residuals', rangemode = "tozero"))
```

### Test Metrics
```{r, fig.width = 10, cache = FALSE}
SST_rf_test <- sum((test_df$n - mean(test_df$n))^2)
SSE_rf_test <- sum((test_df$n - predict(rf_model_2, newdata = test_df))^2)
R_sq_rf_test <- 1-(SSE_rf_test/SST_rf_test)
R_sq_rf_test #0.7298947
```
Our random forest model with training + validation data had an R-squared value of 0.85, while it's performance on test data dropped the R-squared value to 0.73.

## Classification and Regression Tree (CART)  
### Training
```{r, fig.width = 10, cache = FALSE}
#Train CART Model
set.seed(12345)
min_leaf <- 3
min_split <- 3*min_leaf
cart_model <- rpart(n ~ TIMESTEP + MONTH + PERSON_AGE + PERSON_SEX, data=train_df, 
                    control = c(minsplit = min_split, minbucket = min_leaf, cp=0.01)) #default = 0.01
SST_CART_train <- sum((train_df$n - mean(train_df$n))^2)
SSE_CART_train <- sum((train_df$n - predict(cart_model, newdata = train_df))^2)
R_sq_cart_train <- 1-(SSE_CART_train/SST_CART_train)
R_sq_cart_train #0.9473921

#CART Model Validation Metrics
SST_cart_val <- sum((val_df$n - mean(val_df$n))^2)
SSE_cart_val <- sum((val_df$n - predict(cart_model, newdata = val_df))^2)
R_sq_cart_val <- 1-(SSE_cart_val/SST_cart_val)
R_sq_cart_val #-1.753216

# Train_Val CART model
set.seed(12345)
min_leaf <- 3
min_split <- 3*min_leaf
cart_model_2 <- rpart(n ~ TIMESTEP + MONTH + PERSON_AGE + PERSON_SEX, data=train_val_df, 
                    control = c(minsplit = min_split, minbucket = min_leaf, cp=0.01)) #default = 0.01
SST_CART_train_val <- sum((train_val_df$n - mean(train_val_df$n))^2)
SSE_CART_train_val <- sum((train_val_df$n - predict(cart_model_2, newdata = train_val_df))^2)
R_sq_cart_train_val <- 1-(SSE_CART_train_val/SST_CART_train_val)
R_sq_cart_train_val #0.9023342

cart_plot_data <- (data.frame(Variable_Importance = cart_model_2$variable.importance, 
                             Variable_Importance_Pct_Tot = round(100*cart_model_2$variable.importance/sum(cart_model_2$variable.importance),0)) %>% 
                     as.data.frame())
```

### Visualisation
```{r, fig.width = 10, cache = FALSE}
# Make plot for CART Feature Importance
plot_ly(x=cart_plot_data$Variable_Importance_Pct_Tot , y=reorder(cart_plot_data %>% row.names(), cart_plot_data$Variable_Importance_Pct_Tot ), type='bar', orientation = 'h') %>%
  layout(title = "CART Forest Feature Importance", xaxis = list(title = 'Variable_Importance_Pct_Tot'), yaxis = list(title = 'Feature'))

# Make plot for CART diagram
rpart.plot(cart_model_2)

# Make plot for CART Residuals
plot_ly(x=train_val_df$yr_mo, y=(predict(cart_model_2, newdata = train_val_df)-train_val_df$n), type='scatter', mode='markers') %>% 
  layout(title = 'Plot of CART Model Residuals vs yr_mo for Train_Val Data',
         xaxis = list(title = 'yr_mo'),
         yaxis = list(title = 'CART Residuals', rangemode = "tozero"))
```

### Test Metrics
```{r, fig.width = 10, cache = FALSE}
SST_cart_test <- sum((test_df$n - mean(test_df$n))^2)
SSE_cart_test <- sum((test_df$n - predict(cart_model_2, newdata = test_df))^2)
R_sq_cart_test <- 1-(SSE_cart_test/SST_cart_test)
R_sq_cart_test #0.6526338
```
Our cart model with training + validation data had an R-squared value of 0.90, while it's performance on test data dropped the R-squared value to 0.65.

## Extreme Gradient Boosted Tree (XGBoost)  
### Training
```{r, fig.width = 10, cache = FALSE}
# XGB dataset feature changes (factor levels not supported, so integer casting)
train_xgb_df <- (train_df %>% 
                   select(TIMESTEP,MONTH,PERSON_AGE,PERSON_SEX) %>% 
                   mutate(MONTH=as.integer(as.factor(MONTH)),
                          PERSON_SEX=ifelse(PERSON_SEX=='M',0,1)))

val_xgb_df <- (val_df %>% 
                 select(TIMESTEP,MONTH,PERSON_AGE,PERSON_SEX) %>% 
                 mutate(MONTH=as.integer(as.factor(MONTH)),
                        PERSON_SEX=ifelse(PERSON_SEX=='M',0,1)))

train_val_xgb_df <- (train_val_df %>% 
                       select(TIMESTEP,MONTH,PERSON_AGE,PERSON_SEX) %>% 
                       mutate(MONTH=as.integer(as.factor(MONTH)),
                              PERSON_SEX=ifelse(PERSON_SEX=='M',0,1)))

test_xgb_df <- (test_df %>% 
                  select(TIMESTEP,MONTH,PERSON_AGE,PERSON_SEX) %>% 
                  mutate(MONTH=as.integer(as.factor(MONTH)),
                         PERSON_SEX=ifelse(PERSON_SEX=='M',0,1)))

# # #Train & Val Boosted Tree Model - Loop - Run only once to get optimal settings  
# #Train Boosted Tree Model
# bt_results <- data.frame()
# for(i in seq.int(1, 20)){
#   for(j in seq.int(3,6)){
#     set.seed(12345)
#     bt_model <- xgboost(data=as.matrix(train_xgb_df), label = train_df$n, objective='reg:squarederror', nthread=1,  nrounds=i, max.depth=j, eta=0.05)  #
#     summary(bt_model)
#     
#     SST_bt_train <- sum((train_df$n - mean(train_df$n))^2)
#     SSE_bt_train <- sum((train_df$n - predict(bt_model, newdata = as.matrix(train_xgb_df)))^2)
#     R_sq_bt_train <- 1-(SSE_bt_train/SST_bt_train)
#     R_sq_bt_train #0.4761988
#     
#     #Boosted Tree Model Validation Metrics
#     SST_bt_val <- sum((val_df$n - mean(val_df$n))^2)
#     SSE_bt_val <- sum((val_df$n - predict(bt_model, newdata = as.matrix(val_xgb_df)))^2)
#     R_sq_bt_val <- 1-(SSE_bt_val/SST_bt_val)
#     R_sq_bt_val #0.7243043
#     
#     bt_results <- bt_results %>% bind_rows(data.frame(n_trees=i, max_depth=j, R_sq_bt_train=R_sq_bt_train, R_sq_bt_val=R_sq_bt_val))
# 
#   }
# }
# 
# plot_ly(x=bt_results$n_trees, y=bt_results$R_sq_bt_train, type='scatter', mode='lines+markers', color=as.factor(paste0('Train-',bt_results$max_depth))) %>%
#   add_trace(x=bt_results$n_trees, y=bt_results$R_sq_bt_val, type='scatter', mode='lines+markers', color=as.factor(paste0('Val-',bt_results$max_depth))) %>% 
#   layout(title = 'Plot of Boosted Tree Train and Val R_Sq vs Number of Trees (depth in name)',
#          xaxis = list(title = 'Number of Trees'),
#          yaxis = list(title = 'R_Sq'))
# 
# 
# bt_results %>% filter(bt_results$R_sq_bt_val == max(bt_results$R_sq_bt_val))
# # n_trees max_depth R_sq_bt_train R_sq_bt_val
# # 12         5     0.3742168   0.7650132

# Best model from Train/Val tuning
set.seed(12345)
bt_model <- xgboost(data=as.matrix(train_xgb_df), label = train_df$n, objective='reg:squarederror', nthread=1,  nrounds=12, max.depth=5, eta=0.05, verbose=0)
SST_bt_train <- sum((train_df$n - mean(train_df$n))^2)
SSE_bt_train <- sum((train_df$n - predict(bt_model, newdata = as.matrix(train_xgb_df)))^2)
R_sq_bt_train <- 1-(SSE_bt_train/SST_bt_train)
R_sq_bt_train #0.3742168

#Boosted Tree Model Validation Metrics
SST_bt_val <- sum((val_df$n - mean(val_df$n))^2)
SSE_bt_val <- sum((val_df$n - predict(bt_model, newdata = as.matrix(val_xgb_df)))^2)
R_sq_bt_val <- 1-(SSE_bt_val/SST_bt_val)
R_sq_bt_val #0.7650132

# Train_Val Boosted Tree model
set.seed(12345)
bt_model_2 <- xgboost(data=as.matrix(train_val_xgb_df), label = train_val_df$n, objective='reg:squarederror', nthread=1,  nrounds=12, max.depth=5, eta=0.05, verbose=0)
xgb.plot.importance(xgb.importance(model=bt_model_2))

SST_bt_train_val <- sum((train_val_df$n - mean(train_val_df$n))^2)
SSE_bt_train_val <- sum((train_val_df$n - predict(bt_model_2, newdata = as.matrix(train_val_xgb_df)))^2)
R_sq_bt_train_val <- 1-(SSE_bt_train_val/SST_bt_train_val)
R_sq_bt_train_val #0.4057451
```

### Visualisation
```{r, fig.width = 10, cache = FALSE}
plot_ly(x=train_val_df$yr_mo, y=(predict(bt_model_2, newdata = as.matrix(train_val_xgb_df))-train_val_df$n), type='scatter', mode='markers') %>%
  layout(title = 'Plot of Boosted Tree Model Residuals vs yr_mo for Train_Val Data',
         xaxis = list(title = 'yr_mo'),
         yaxis = list(title = 'BT Residuals', rangemode = "tozero"))
```

### Test Metrics
```{r, fig.width = 10, cache = FALSE}
SST_bt_test <- sum((test_df$n - mean(test_df$n))^2)
SSE_bt_test <- sum((test_df$n - predict(bt_model_2, newdata = as.matrix(test_xgb_df)))^2)
R_sq_bt_test <- 1-(SSE_bt_test/SST_bt_test)
R_sq_bt_test #0.2349657
```
Our XGBoost model with training + validation data had an R-squared value of 0.41, while it's performance on test data dropped the R-squared value to 0.23.

## Ensemble Model  
### Training
```{r, fig.width = 10, cache = FALSE}
# Ensemble final models for better performance
val_pred_actual <- val_df$n
val_pred_lm <- predict(lm_model_2, newdata = val_df)
val_pred_rf <- predict(rf_model_2, newdata = val_df)
val_pred_cart <- predict(cart_model_2, newdata = val_df)
val_pred_bt <- predict(bt_model_2, newdata = as.matrix(val_xgb_df))

ensemble_model <- lm(val_pred_actual ~ val_pred_lm + val_pred_rf + val_pred_cart + val_pred_bt)
summary(ensemble_model)

#  =>  ensemble model will be -15.493854 + 1.746018*bt_model_2 + 0.378762*rf_model_2 - 0.088279*cart_model_2

# Ensemble model result calculations
val_pred_ensemble <- -15.493854 + 1.746018*val_pred_bt + 0.378762*val_pred_rf - 0.088279*val_pred_cart

SST_ensemble_val <- sum((val_df$n - mean(val_df$n))^2)
SSE_ensemble_val <- sum((val_df$n - val_pred_ensemble)^2)
R_sq_ensemble_val <- 1-(SSE_ensemble_val/SST_ensemble_val)
R_sq_ensemble_val #0.9139694

train_val_pred_ensemble <- -15.493854 + 1.746018*predict(bt_model_2, newdata = as.matrix(train_val_xgb_df)) + 0.378762*predict(rf_model_2, newdata = train_val_df) - 0.088279*predict(cart_model_2, newdata = train_val_df)
SST_ensemble_train_val <- sum((train_val_df$n - mean(train_val_df$n))^2)
SSE_ensemble_train_val <- sum((train_val_df$n - train_val_pred_ensemble)^2)
R_sq_ensemble_train_val <- 1-(SSE_ensemble_train_val/SST_ensemble_train_val)
R_sq_ensemble_train_val #0.9624101

ensemble_plot_data <- data.frame(n_ensemble = as.numeric(train_val_pred_ensemble), n=train_val_df$n, yr_mo=train_val_df$yr_mo)
```

### Visualisation
```{r, fig.width = 10, cache = FALSE}
plot_ly(x=ensemble_plot_data$yr_mo, y=(ensemble_plot_data$n_ensemble - ensemble_plot_data$n), type='scatter', mode='markers') %>%
  layout(title = 'Plot of Ensemble Model Residuals vs yr_mo for Train_Val Data',
         xaxis = list(title = 'yr_mo'),
         yaxis = list(title = 'Ensemble Residuals', rangemode = "tozero"))
```

### Test Metrics
```{r, fig.width = 10, cache = FALSE}
test_pred_ensemble <- -15.493854 + 1.746018*predict(bt_model_2, newdata = as.matrix(test_xgb_df))  + 0.378762*predict(rf_model_2, newdata = test_df) - 0.088279*predict(cart_model_2, newdata = test_df)
SST_ensemble_test <- sum((test_df$n - mean(test_df$n))^2)
SSE_ensemble_test <- sum((test_df$n - test_pred_ensemble)^2)
R_sq_ensemble_test <- 1-(SSE_ensemble_test/SST_ensemble_test)
R_sq_ensemble_test #0.8646812
```
Our Ensemble model with training + validation data had an R-squared value of 0.96, while it's performance on test data dropped the R-squared value to 0.86.


# Model comparison
Each model's performance can be compared in the following interactive plot.  The red line represents a perfect model.  Notice how much better the ensemble model does than the other individual models.

## Test Data
```{r, fig.width = 10}
plot_ly(x=test_df$n, y=(predict(cart_model_2, newdata = test_df)), type='scatter', mode='markers', name='CART', marker=list(color='#1f77b4', opacity=0.5)) %>% 
  add_trace(x=test_df$n, y=(predict(rf_model_2, newdata = test_df)), type='scatter', mode='markers', name='RF', marker=list(color='#ff7f0e', opacity=0.5)) %>% 
  add_trace(x=test_df$n, y=(predict(lm_model_2, newdata = test_df)), type='scatter', mode='markers', name='LM', marker=list(color='#2ca02c', opacity=0.5)) %>%
  add_trace(x=c(0,350), y=c(0,350), type='scatter', mode='lines+markers', name='Perfect', alpha=1, line=list(color='#d62728'), marker=list(color='#2ca02c', opacity=0)) %>% 
  add_trace(x=test_df$n, y=(predict(bt_model_2, newdata = as.matrix(test_xgb_df))), type='scatter', mode='markers', name='BT', marker=list(color='#9467bd', opacity=0.5)) %>%
  add_trace(x=test_df$n, y=(test_pred_ensemble), type='scatter', mode='markers', name='Ensemble', marker=list(color='#8c564b', opacity=0.5)) %>%
  layout(title = 'Plot of All Model Predictions vs Actual Values - Test Data',
         xaxis = list(title = 'Actual Value', range=c(0,375)),
         yaxis = list(title = 'Model Prediction', range=c(-200,350)))
```

## Overall Model Performance Summary Table
```{r, fig.width = 10, cache = FALSE, message=FALSE}
kable(data.frame(Model_Type = c('Linear', 'Random Forest', 'CART', 'Boosted Trees', 'Ensemble'),
                 R_sq_train = round(c(R_sq_lm_train, R_sq_rf_train, R_sq_cart_train, R_sq_bt_train, as.integer(NA)), 2),
                 R_sq_val = round(c(R_sq_lm_val, R_sq_rf_val, R_sq_cart_val, R_sq_bt_val, R_sq_ensemble_val), 2),
                 R_sq_train_val = round(c(R_sq_lm_train_val, R_sq_rf_train_val, R_sq_cart_train_val, R_sq_bt_train_val, R_sq_ensemble_train_val), 2),
                 R_sq_test = round(c(R_sq_lm_test, R_sq_rf_test, R_sq_cart_test, R_sq_bt_test, R_sq_ensemble_test), 2))) %>% 
  kable_classic(full_width = FALSE, html_font = "Cambria") %>%
  row_spec(c(5), background = c("yellow"))
```

```{r include=FALSE}
body_shop_count <- 710
```

We can see that our Ensemble model has the best performance on the Train + Validation data, so we will move forward with this model. We will now estimate the monthly volume our auto body repair center can expect in the future, using our expected market share of `r percent(1/body_shop_count, .0001)`.

We therefore select the Ensemble model due to its superior performance.

# Prediction
We continue with the assumption here is that our repair shop will enjoy perfect competition. This would mean that it would have an equal share of the demand i.e. `r percent(1/body_shop_count, .0001)`. With this assumption in tow, we estimate the shop's future monthly car volume.

## Computation
First, we formally compute the market share of the shop.
```{r, fig.width = 10, cache = FALSE}
body_shop_count <- 710
Shop_Market_Share <- 0.1408/100
```
Now, we use the test data that we had set aside to predict the monthly collision volume.
```{r, fig.width = 10, cache = FALSE}
test_agg_df <- (test_df %>% 
                  group_by(MONTH) %>% 
                  summarise(Actual_NYC_Collisions=sum(n), 
                            Actual_Shop_Volume=round(Shop_Market_Share*sum(n), 0)))

```
Here, we create dataset for future year (2021).
```{r, fig.width = 10, cache = FALSE}
temp1 <- data.frame(TIMESTEP = c(48:59))
temp2 <- data.frame(MONTH = c('01','02','03','04','05','06','07','08','09','10','11','12'))
temp3 <- data.frame(PERSON_AGE = c(15:100))
temp4 <- data.frame(PERSON_SEX = c('M','F'))
monthly_predictions_df <- temp1 %>%
  bind_cols(temp2) %>%
  full_join(temp3, by=character()) %>%
  full_join(temp4, by=character())
```
And finally, we make predictions.
```{r, fig.width = 10, cache = FALSE}
monthly_predictions_xgb_df <- (monthly_predictions_df %>% 
                                 select(TIMESTEP,MONTH,PERSON_AGE,PERSON_SEX) %>% 
                                 mutate(MONTH=as.integer(as.factor(MONTH)),
                                        PERSON_SEX=ifelse(PERSON_SEX=='M',0,1)))
#Create predictions
monthly_predictions_df$rf <- predict(rf_model_2, newdata = monthly_predictions_df)
monthly_predictions_df$cart <- predict(cart_model_2, newdata = monthly_predictions_df)
monthly_predictions_df$bt <- predict(bt_model_2, newdata = as.matrix(monthly_predictions_xgb_df))
monthly_predictions_df$ensemble <- (-15.493854 
                                    + 1.746018*monthly_predictions_df$bt  
                                    + 0.378762*monthly_predictions_df$rf 
                                    - 0.088279*monthly_predictions_df$cart)

monthly_predictions_agg_df <- (monthly_predictions_df %>% 
                                 group_by(MONTH) %>% 
                                 summarise(Predicted_NYC_Collisions=round(sum(ensemble), 0)) %>%
                                 mutate(Predicted_Shop_Volume = round(Shop_Market_Share*Predicted_NYC_Collisions, 0)) %>% 
                                 left_join(test_agg_df, by='MONTH') %>% 
                                 mutate(YEAR = 2021) %>% 
                                 select(YEAR, MONTH, Actual_NYC_Collisions, Actual_Shop_Volume, Predicted_NYC_Collisions, Predicted_Shop_Volume)
)   

kable(monthly_predictions_agg_df) %>% 
  kable_classic(full_width = FALSE, html_font = "Cambria")
```

## Plot of actual versus predicted demand volume
```{r, fig.width = 10, cache = FALSE, warning=FALSE}
#Plot of Actual_Shop_Volume and Predicted_Shop_Volume
plot_ly(x=monthly_predictions_agg_df$MONTH, y=monthly_predictions_agg_df$Actual_Shop_Volume, 
        type='bar', name='Actual_Shop_Volume') %>% 
  add_trace(x=monthly_predictions_agg_df$MONTH, y=monthly_predictions_agg_df$Predicted_Shop_Volume, type='bar', name='Predicted_Shop_Volume') %>% 
  layout(title = 'Plot of Actual_Shop_Volume and Predicted_Shop_Volume (Test Set)',
         xaxis = list(title = 'Months in 2021'),
         yaxis = list(title = 'Car volume'))
```

# Primary research question response
Recall that our [primary research question](#primary-hypotheses) is to "estimate the monthly demand for a fictitious auto shop in New York state by extrapolating the monthly car crash volume with reasonable accuracy". We can see that our ensemble model actually does a very good job predicting the shop volume, compared the actual volume the shop saw in 2021. We hypothesize that our model would have performed even better without the unforeseen consequences of COVID-19 and the subsequent increase of remote work availability.  

Our next task is to start answering our secondary research objectives, including identifying key demographics for use in a marketing campaign and then measuring its cost and effect.

## Proportion of crashes by month, gender
```{r, fig.width = 10, cache = FALSE, warning=FALSE}
kable(train_df %>% 
        group_by(PERSON_SEX) %>% 
        summarise(n = sum(n)) %>% 
    arrange(PERSON_SEX)) %>% 
  kable_classic(full_width = FALSE, html_font = "Cambria")
```

## Proportion of crashes by month, age group
```{r, fig.width = 10, cache = FALSE, warning=FALSE}
kable(train_df %>% 
        mutate(age_group = 5*floor(PERSON_AGE/5)) %>%
        group_by(age_group) %>% 
        summarise(n = sum(n)) %>% 
        arrange(age_group)) %>% 
  kable_classic(full_width = FALSE, html_font = "Cambria")
```

# Marketing Campaign
With the assumption that the auto body repair shop has a 0.14% share of the NYC business, we set a goal to increase it to about 0.25%.  To do so, we considered a few options and settled on a marketing campaign that focuses on digital search advertisement. 

We started off by pulling together the monthly numbers from 2021 and added a new column for the revenue we expect to generate. Based on information from the 2017 **Shop Performance Survey** (@TravisBen), the average repair in the United States costs \$200-\$399.  However, we chose to use \$2400 as the average repair order as collision repairs are significantly higher. The same source also listed the average gross profit and and net profit margins (49% and 14% respectively) for similar shops in the United States. Below is a table showing the calculations.
```{r}
#Average Repair Shop Financial Metrics
average_repair_order <- 2400  #Average repair is about $399
gross_profit_margin <- .49
net_profit_margin <- .14

marketing_campaign_df <- data.frame(monthly_predictions_agg_df)

marketing_campaign_df <- marketing_campaign_df %>%
                          mutate(monthly_revenue = marketing_campaign_df$Predicted_Shop_Volume* average_repair_order)

marketing_campaign_df %>%
  kbl() %>%
  kable_paper("hover", full_width = F, html_font = "Cambria")
```

We also obtained digital advertising data from the **2021 Paid Search Advertising Benchmarks for Every Industry** article by Kristen McCormick (@KristenMcCormick).  We noticed that both the click-through and conversion rate, 5.39% and 15.23% respectively, were very good and presented a real opportunity to archive our goal of increasing the market share.  For comparison, we ran the numbers based on three levels of spend ($200, $300, and $400).  We summarized the calculations in the table below.

```{r}
# 2021 Paid Search Industry Benchmarks (from Kristen McCormick article)
CTR <- 0.0539  #Average click-through rate 5.39%
CPC <- 3.19  #Average cost per click
conversion_rate <- 0.1523  #Average conversion rate is 15.23%
cost_per_lead <- 17.81

#Create a function that will create a column for each level of spend
create_col <- function(spend) {
  clicks <- spend/CPC
  conversion <- clicks*conversion_rate
  revenue <- conversion*average_repair_order
  ROAS <- revenue/spend
  market_share_lif <- conversion / marketing_campaign_df$Predicted_NYC_Collisions[12] * 100
  grs_prf <- revenue * gross_profit_margin
  net_prft <- revenue * net_profit_margin
  ret_on_inv <- net_prft/spend
  
  round(c(clicks, conversion, revenue, ROAS, market_share_lif, grs_prf, net_prft, ret_on_inv),2)
}

#create and display a dataframe for the three levels of spend
kable(data.frame(Campaign_KPIs = c('Clicks' , 'Conversion' , 'Revenue' , 'ROAS' , 
                                   'Market Share Lift' , 'Gross Profit' , 'Net Profit' , 'ROI'),
                 'USD200' = create_col(200),
                 'USD300' = create_col(300),
                 'USD400' = create_col(400))) %>% 
  kable_paper("hover", full_width = F, html_font = "Cambria") %>%
  column_spec(3, color =c("white"), background = "green")

```

From the summary, it appears that spending $300 per month on digital advertising will allow us to get very close to our goal of 0.25% of the market share.

# Effects of COVID-19 on NYC Vehicle Collisions
The first reported case of COVID-19 was confirmed in New York on March 1, 2020, while later reports suggest that the first case could have been as early as January. Researchers later estimated that at the time the first case was confirmed, as many as 10,700 New Yorkers had already contracted the virus (@CarolineLewis). By March 9, there were 16 confirmed cases in NYC alone and on March 16, Governor Andrew Cuomo announced public schools would be closed, effectively beginning the COVID-19 lockdown. This remained in effect until June 8, when the governor announced the first phase of reopening under safety protocols. By the end of 2020, researchers estimate that 44% of all metro New York residents had been infected. To date, over 35,000 deaths of New York citizens have been attributed to COVID-19, with at least another 5,000 listed as probable deaths (@nycgov).

```{r, fig.width = 10, cache = FALSE}
#filter data, create combined_df2
combined_df2 <- (crashes_df %>% 
                  select(-c(CRASH_DATE, CRASH.TIME, CONTRIBUTING.FACTOR.VEHICLE.1, CONTRIBUTING.FACTOR.VEHICLE.2,
                            CONTRIBUTING.FACTOR.VEHICLE.3, CONTRIBUTING.FACTOR.VEHICLE.4, CONTRIBUTING.FACTOR.VEHICLE.5,
                            VEHICLE.TYPE.CODE.4, VEHICLE.TYPE.CODE.5,
                            NUMBER.OF.PEDESTRIANS.INJURED, NUMBER.OF.PEDESTRIANS.KILLED,
                            NUMBER.OF.CYCLIST.INJURED, NUMBER.OF.CYCLIST.KILLED,
                            ON.STREET.NAME, CROSS.STREET.NAME, OFF.STREET.NAME)) %>% 
                  inner_join(vehicles_df %>% 
                               filter(!is.na(VEHICLE_ID)) %>% 
                               select(-c(CRASH_DATE, CRASH_TIME, VEHICLE_ID))
                             , by = 'COLLISION_ID') %>% 
                  inner_join(person_df %>% 
                               select(-c(UNIQUE_ID, CONTRIBUTING_FACTOR_1, CONTRIBUTING_FACTOR_2))
                             , by= c('COLLISION_ID'='COLLISION_ID', 'UNIQUE_ID'='VEHICLE_ID')) %>% 
                  filter((PED_ROLE %in% c('Driver'))) %>%  #drivers only
                  filter((CRASH_DATE >= '2019-01-01') & (CRASH_DATE < '2020-12-31')) %>% 
                   #only from 2017-01-01 to 2021-11-30
                  filter(PERSON_AGE > 14 & PERSON_AGE < 101) %>%
                  filter(PERSON_SEX == 'F' | PERSON_SEX == 'M') %>%
                  mutate(DAY = substr(CRASH_DATE,9,10)) %>%
                  mutate(TIMESTEP2 = as.period(interval(as.Date('2019-01-01'), CRASH_DATE)) %/% days(1)) %>% 
                   ####add TIMESTEP2
                  mutate(yr_mo_day = paste0(substr(CRASH_DATE,1,4),'-',substr(CRASH_DATE,6,7),'-',substr(CRASH_DATE,9,10))) %>% 
                   ######add yr_mo_day
                  select(COLLISION_ID, CRASH_DATE, DAY, TIMESTEP2, yr_mo_day, NUMBER.OF.PERSONS.KILLED, 
                         PERSON_AGE, PERSON_SEX) %>% 
                   ####ADD KILLED, DAY, TIMESTEP2, yr_mo_day
                  arrange(CRASH_DATE)
                )
kable(t(summary(combined_df2))) %>% kable_classic(full_width = TRUE, html_font = "Cambria", font_size = 14)
```

## Explore Crash Data by Day, 2019-2020
```{r, fig.width = 10, cache = FALSE}
daily_agg1 <- (combined_df2 %>% 
                   group_by(TIMESTEP2, yr_mo_day, DAY) %>% 
                   summarise(n = n(), .groups = 'drop') %>% 
                   arrange(TIMESTEP2, yr_mo_day, DAY)
                 )
plot_ly(x=daily_agg1$yr_mo_day, y=daily_agg1$n, type='bar') %>%
  layout(title = "Crash Data by Day from 2019-2020", xaxis = list(title = 'Day'), yaxis = list(title = 'Number of Crashes'))
```
```{r, fig.width = 10, cache = FALSE}
#create covid_df for COVID analysis
covid_df <- (combined_df2 %>% 
                   filter((CRASH_DATE >= '2019-01-01') & (CRASH_DATE < '2020-12-31')) %>% #only from 2019-01-01 to 2020-12-31))
                   mutate(covid = ifelse(CRASH_DATE >= '2020-03-16', 1, 0)) %>%
                   rename(fatalities = NUMBER.OF.PERSONS.KILLED) %>%
                   group_by(TIMESTEP2, yr_mo_day, DAY, covid, fatalities) %>% 
                   summarise(n = n(), .groups = 'drop') %>% 
                   arrange(TIMESTEP2, yr_mo_day, DAY, covid, fatalities)
)
head(covid_df)
```

## Linear Regression with Indicator Variable

$$
Crashes = \beta_0 + \beta_1*Covid
$$

We used COVID as an indicator variable and the number of daily crashes in NYC as the response variable. The base case when the variable covid=0 represents January 1, 2019 - March 15, 2020. When covid=1, this represents March 16, 2020 - Dec 31, 2020.

```{r}
#linear regression with only covid indicator variable
covid_model1 <- lm(n ~ covid, data = covid_df)
summary(covid_model1)
```

Both the intercept and the B1 values are statistically significant. We can conclude from this model that before COVID, there was an average of 602 crashes a day in NYC. This number dropped to 233 crashes a day after COVID. This shows a large and significant decrease in the number of overall crashes by 61.3%.

According to the National Highway Traffic Safety Administration from the US Department of Transportation, there are 13.6 million vehicle collisions each year in the United State and the average cost of vehicle repair per collision is $21,036 (@SRBarnes). Our research also indicates that 22 percent of collision claims involve vehicles that are totaled (@MarkVallet). We assume that accidents not totaled will be potential business for our auto body repair center. We then conclude that the average accidents a day, removing totaled collisions, from 2019 though the beginning of COVID-19 is 470, while the average accidents a day after COVID-19 through the end of 2020 is 182. 

We estimate 6,054,581 dollars in lost revenue for auto body shops in NYC post COVID. Assuming .14% market share for our fictitious auto body shop, lost revenue totals 8,476 dollars a day. Furthermore, we estimate that the total revenue lost during the COVID-19 lockdown in NYC from March - June 2020 is 712,019 dollars.

# Future work
To build on this work, one can perform further data analysis to

- Understand differences between traffic collisions that involve cyclists and those that do not. Research by Li and Zhao points to significant differences between cyclists and drivers experiences of vehicle collisions with cyclists experiencing 3 times as many collisions as drivers with fatal outcomes.
- Determine features of vehicular and cycling collision hot spots.
- Use the sophisticated formulae for traffic exposure from Regev et. al. (133) on new data sets to see if it applies in other places.
- Perform more data exploration through advanced visualizations to pull out and analyze any other trends in the data.

# Works Cited